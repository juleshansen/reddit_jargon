{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitbaseconda42db193651bf4e5fab9381b4edefb3d8",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "448eea69390e87439a5333a90a303e7024d2ba171b1cfbbf9a993ee52ea35a5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gensim Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "import contractions\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_dict = set(words.words())\n",
    "lemma = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2 = pd.read_csv('../data/comments.csv', sep='|')\n",
    "comments2.dropna(inplace=True)\n",
    "text = list(comments2['text'])\n",
    "strip_punctuation = re.compile('[%s]' % re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def expand_contractions(corpus):\n",
    "#     expanded = [contractions.fix(comment.lower()) for comment in corpus]\n",
    "#     return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_punctuation(corpus):\n",
    "#     strip_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "#     no_punc = [strip_punc.sub('', comment) for comment in corpus]\n",
    "#     return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessCorpus():\n",
    "    '''\n",
    "    Class for NLP processign to create a corpus that has:\n",
    "      - contractions expanded\n",
    "      - punctuation stripped\n",
    "      - lemmatizaion\n",
    "    There is an optional frequency filter included to remove words that appear less than some number of times.\n",
    "\n",
    "    This class has stored attributes for a dictionary and corpus object to be supplied to gensim.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, stopwords=None, eng_dict=None):\n",
    "        self.stopwords = stopwords\n",
    "        self.eng_dict = eng_dict\n",
    "    \n",
    "    def __expand_contractions(self):\n",
    "        '''\n",
    "        Expands contraction words such as don't --> do not prior to removing punctuation so meaning isn't lost and words will still appear in supplied dictionary for dictionary filtering.\n",
    "        '''\n",
    "        expanded = [contractions.fix(comment.lower()) for comment in self.corpus]\n",
    "        return expanded\n",
    "\n",
    "    def __strip_punctuation(self):\n",
    "        '''\n",
    "        Removes all punctuation from corpus.\n",
    "        '''\n",
    "        strip_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        no_punc = [strip_punc.sub('', comment) for comment in self.corpus]\n",
    "        return no_punc\n",
    "\n",
    "    def __lemmatize(self):\n",
    "        '''\n",
    "        Lemmatizes words using NLTK package's WordNetLemmatizer.\n",
    "        '''\n",
    "        split = []\n",
    "        lemma = WordNetLemmatizer()\n",
    "        for comment in self.corpus:\n",
    "            split.append([lemma.lemmatize(word) for word in comment.split() if word not in self.stopwords])\n",
    "        return split\n",
    "\n",
    "    def __frequency_dict(self):\n",
    "        '''\n",
    "        Creates a dictionary of word frequencies in the corpus to be used for frequency filtering. Good for removing typos, and words that are not frequent enough to be considered slang due to low adoption by subculture.\n",
    "        '''\n",
    "        self.frequency_dict = defaultdict(int)\n",
    "        for text in self.corpus:\n",
    "            for token in text:\n",
    "                self.frequency_dict[token] += 1\n",
    "        \n",
    "    def __filter_english(self):\n",
    "        self.no_eng = [\n",
    "            [word for word in comment\n",
    "            if word not in self.eng_dict]\n",
    "            for comment in self.corpus\n",
    "        ]\n",
    "\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        '''\n",
    "        Call to process corpus.\n",
    "\n",
    "        Creates attributes:\n",
    "        corpus - processed\n",
    "        gensim_dictionary\n",
    "        '''\n",
    "        self.corpus = corpus\n",
    "        self.corpus = self.__expand_contractions()\n",
    "        self.corpus = self.__strip_punctuation()\n",
    "        self.corpus = self.__lemmatize()\n",
    "        self.__frequency_dict()\n",
    "        self.gensim_dictionary = gensim.corpora.Dictionary(self.corpus)\n",
    "        temp = self.gensim_dictionary[0]\n",
    "        self.gensim_corpus = [self.gensim_dictionary.doc2bow(text) for text in self.corpus]\n",
    "        \n",
    "        self.__filter_english()\n",
    "        self.gensim_dictionary_no_english = gensim.corpora.Dictionary(self.no_eng)\n",
    "        temp = self.gensim_dictionary_no_english[0]\n",
    "        self.gensim_corpus_no_english = [self.gensim_dictionary_no_english.doc2bow(text) for text in self.no_eng]\n",
    "\n",
    "    def return_frequency_filter(self, frequency = 1):\n",
    "        freq_filter = [\n",
    "            [token for token in text if self.frequency_dict[token] > frequency]\n",
    "            for text in self.corpus\n",
    "        ]\n",
    "        return freq_filter\n",
    "    \n",
    "    def apply_frequency_filter(self, frequency = 1):\n",
    "        self.corpus_freq = self.return_frequency_filter(frequency)\n",
    "        self.gensim_dictionary_freq = gensim.corpora.Dictionary(self.corpus)\n",
    "        temp = self.gensim_dictionary_freq[0]\n",
    "        self.gensim_corpus_freq = [self.gensim_dictionary.doc2bow(text) for text in self.corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(corpus.corpus)\n",
    "c = [dictionary.doc2bow(text) for text in corpus.corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ProcessCorpus(stopwords=stops, eng_dict=eng_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.apply_frequency_filter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel(\n",
    "    corpus=corpus.gensim_corpus,\n",
    "    id2word=corpus.gensim_dictionary.id2token,\n",
    "    chunksize=2000,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=400,\n",
    "    num_topics=10,\n",
    "    passes=20,\n",
    "    eval_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 120600)"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "model.get_topics().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mdl/model2/full_corpus_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "toptopics = model.top_topics(corpus.gensim_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([(0.027945692, 'I'),\n",
       "  (0.020974234, 'would'),\n",
       "  (0.016574621, 'like'),\n",
       "  (0.01620022, 'year'),\n",
       "  (0.015440419, 'people'),\n",
       "  (0.013655773, 'get'),\n",
       "  (0.012080382, 'time'),\n",
       "  (0.011261409, 'think'),\n",
       "  (0.010394038, 'one'),\n",
       "  (0.00971735, 'make'),\n",
       "  (0.009337797, 'work'),\n",
       "  (0.008740346, 'much'),\n",
       "  (0.008324937, 'going'),\n",
       "  (0.008266438, 'good'),\n",
       "  (0.008090082, 'go'),\n",
       "  (0.0080230115, 'want'),\n",
       "  (0.007572819, 'know'),\n",
       "  (0.0073177647, 'even'),\n",
       "  (0.007150487, 'really'),\n",
       "  (0.0070460634, 'need')],\n",
       " -2.1333582068623778)"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "toptopics[0]"
   ]
  }
 ]
}