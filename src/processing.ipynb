{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitbaseconda42db193651bf4e5fab9381b4edefb3d8",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "448eea69390e87439a5333a90a303e7024d2ba171b1cfbbf9a993ee52ea35a5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Processing Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "source": [
    "## Outline for data processing as outlined in paper:\n",
    " 1. Remove Stopwords\n",
    " 2. Frequency Filtering\n",
    " 3. Dictionary-based Filtering\n",
    " 4. Topic Checking\n",
    " 5. Embedding Expansion\n",
    " 6. Embedding Filtering\n",
    " 7. User Usage Filtering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('../data/comments_general.csv')\n",
    "comments.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK has \n",
    "eng_dict = set(words.words())\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'No but HUD really will only help single mothers, pregnant and expecting women, the disabled, and veterans, if you’re not one of those you’re about to have a LONG wait..'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "comments['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['no',\n",
       " 'but',\n",
       " 'hud',\n",
       " 'really',\n",
       " 'will',\n",
       " 'only',\n",
       " 'help',\n",
       " 'single',\n",
       " 'mothers,',\n",
       " 'pregnant',\n",
       " 'and',\n",
       " 'expecting',\n",
       " 'women,',\n",
       " 'the',\n",
       " 'disabled,',\n",
       " 'and',\n",
       " 'veterans,',\n",
       " 'if',\n",
       " 'you’re',\n",
       " 'not',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'you’re',\n",
       " 'about',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'long',\n",
       " 'wait..']"
      ]
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "[lemma.lemmatize(word) for word in comments['text'][0].lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "# no_stops = comments.apply(lambda x: [for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(stop_words=stops, strip_accents='unicode')\n",
    "word_document_matrix = cntvec.fit_transform(comments['text'][:50])\n",
    "features = cntvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<50x329 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 329 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "mask = [word not in eng_dict for word in features]\n",
    "feat_mask = np.array(features)[mask]\n",
    "word_document_matrix[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['1st', '3x9', 'accounts', 'arrived', 'balls', 'bombay', 'bored',\n",
       "       'btw', 'caps', 'cats', 'chillin', 'clasps', 'complaints', 'crows',\n",
       "       'damaged', 'decor', 'defo', 'died', 'dropped', 'drowned', 'eco',\n",
       "       'ends', 'etc', 'expecting', 'favourite', 'gazillion', 'gets',\n",
       "       'gme', 'helps', 'hemming', 'horses', 'hunted', 'hunter', 'inches',\n",
       "       'instagram', 'intestines', 'iphone', 'keeps', 'kitties', 'knees',\n",
       "       'liked', 'lives', 'lol', 'looks', 'loved', 'makes', 'meows',\n",
       "       'michael', 'mothers', 'needed', 'omggggg', 'online', 'op',\n",
       "       'origional', 'packaged', 'pairs', 'patches', 'pendants', 'phrased',\n",
       "       'pieces', 'pleased', 'poo', 'posts', 'pouri', 'prices', 'products',\n",
       "       'realises', 'reminds', 'scott', 'scrubs', 'scrunchie', 'seems',\n",
       "       'shreds', 'snacks', 'soooo', 'straps', 'subscribed', 'tears',\n",
       "       'things', 'threads', 'toys', 'treats', 'trekked', 'tubes', 'uhaul',\n",
       "       'veterans', 'vets', 'wanted', 'women', 'worms'], dtype='<U12')"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "feat_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LatentDirichletAllocation()"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "reddit_lda = LatentDirichletAllocation(n_components=10)\n",
    "reddit_lda.fit(word_document_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_out = reddit_lda.transform(word_document_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4, 0, 0, 7, 0, 9, 8, 0, 0, 0, 8, 0, 1, 7, 2, 9, 0, 7, 9, 5, 7, 1,\n",
       "       9, 7, 6, 4, 5, 5, 0, 9, 4, 5, 6, 1, 7, 5, 9, 4, 3, 2, 5, 6, 8, 0,\n",
       "       4, 4, 3, 9, 9, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "lda_out.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       ...,\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.1, 0.1, ..., 2.1, 0.1, 1.1]])"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "reddit_lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['company', 'cat', 'cat', 'cat', 'cat', 'bird', 'dollar', 'looks',\n",
       "       'context', 'like'], dtype='<U12')"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "np.array(features)[np.argmax(reddit_lda.components_, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "looks       2.100019\none         1.100002\nbought      1.100002\ngreat       1.100002\narrived     1.100001\nbroken      1.100000\nwanted      1.100000\ncame        1.100000\npendants    1.100000\ncircular    1.100000\nName: 0, dtype: float64\ncompany       3.100000\nswimsuit      2.100001\nknow          2.100000\nmeows         2.100000\nstill         1.100001\nconsidered    1.100000\nprotect       1.100000\nproducts      1.100000\nclasps        1.100000\ncomplaints    1.100000\nName: 1, dtype: float64\ncat          3.099867\nhelp         2.100000\ngood         1.100004\nwomen        1.100000\nwait         1.100000\ndisabled     1.100000\nsingle       1.100000\nmothers      1.100000\nveterans     1.100000\nexpecting    1.100000\nName: 2, dtype: float64\ncat        2.100036\nthought    1.100008\ntaking     1.100006\nbetter     1.100004\nlove       1.100001\nlong       1.100001\nstraps     1.100000\ncut        1.100000\ntied       1.100000\ntime       1.100000\nName: 3, dtype: float64\nmaterial    1.100013\nway         1.100010\nneeded      1.100001\nuse         1.100001\nhand        1.100001\npatches     1.100000\ntears       1.100000\nexact       1.100000\nknees       1.100000\npatch       1.100000\nName: 4, dtype: float64\ncat          5.100091\nlol          3.100000\ntoys         2.100004\nalso         2.100001\npaper        1.100002\nidea         1.100001\nwell         1.100000\nthings       1.100000\nloved        1.100000\npreferred    1.100000\nName: 5, dtype: float64\ncat        2.100029\nlike       2.100017\nwant       2.100000\nrip        2.100000\nkitties    1.100008\ntoy        1.100001\nblack      1.100000\nmaking     1.100000\nnew        1.100000\ngo         1.100000\nName: 6, dtype: float64\nbird        2.100005\nthought     2.099993\nlike        1.100004\nwould       1.100001\ntaken       1.100001\nback        1.100000\nwhenever    1.100000\ndropped     1.100000\ngone        1.100000\nusually     1.100000\nName: 7, dtype: float64\ncat        2.100065\nthreads    2.100000\nthought    1.100017\nthing      1.100014\nfall       1.100003\nnever      1.100003\nlet        1.100001\nbtw        1.100000\nloose      1.100000\nhabit      1.100000\nName: 8, dtype: float64\nget       2.100004\ndogs      2.100003\npaper     1.100007\nmake      1.100003\nshred     1.100003\nsuper     1.100002\nclose     1.100000\nends      1.100000\nsnacks    1.100000\ntreats    1.100000\nName: 9, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(pd.DataFrame(reddit_lda.components_, columns=features).T.sort_values(i, ascending=False)[i][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = gensim.models.Doc2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(comments['text'])\n",
    "corpus = [comment.lower().split() for comment in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['no',\n",
       " 'but',\n",
       " 'hud',\n",
       " 'really',\n",
       " 'will',\n",
       " 'only',\n",
       " 'help',\n",
       " 'single',\n",
       " 'mothers,',\n",
       " 'pregnant',\n",
       " 'and',\n",
       " 'expecting',\n",
       " 'women,',\n",
       " 'the',\n",
       " 'disabled,',\n",
       " 'and',\n",
       " 'veterans,',\n",
       " 'if',\n",
       " 'you’re',\n",
       " 'not',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'you’re',\n",
       " 'about',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'long',\n",
       " 'wait..']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "corpus[0]"
   ]
  }
 ]
}